# AI와 테스트를 활용한 안정적인 기능 개발 리포트

## 사용하는 도구를 선택한 이유가 있을까요? 각 도구의 특징에 대해 조사해본적이 있나요?

이번 과제는 비용을 최소화하면서도 안정적인 AI 개발 환경을 구축하는 것을 목표로 시작했습니다.
회사에서 GitHub Copilot을 기본적으로 제공하고 있고, Google Gemini CLI는 분당 60회, 하루 1000회까지 무료 요청이 가능하고, Cursor는 7일간 무료 체험이 제공되어 이 세 가지 중에서 어떤 도구를 사용할지 고민했습니다.

**GitHub Copilot (GPT-5)**

장점

- VS Code/JetBrains 완벽 통합
- GPT-5 기반으로 코드 품질 높음
- 프로젝트 컨텍스트 이해력 우수

한계

- 세션 컨텍스트 지속성이 약해 긴 대화형 작업에 비효율
- 설계 방향 어긋날 경우 토큰 낭비 발생

**Gemini CLI (Pro)**

장점

- 빠른 응답 속도, 로컬 CLI로 편하게 실행 가능
- 일일 1000회 무료 요청으로 테스트에 적합
- 구글 생태계(GCP, Colab 등)와 연동 쉬움

한계

- 모델 일관성이 낮고, 맥락 유지가 어려움
- 요청 제한(쿼터)으로 장시간 개발엔 부적합

**Cursor (Sonnet 4.5)**

장점

- 코드 품질 및 맥락 유지력 우수
- 프로젝트 파일 전체를 분석해 코드 수정 정확도 높음
- 자동 리팩터링 및 문맥 추론 능력 탁월

한계

- 체험 이후 유료 전환 필요
- 초기 세팅 시 Copilot보다 약간 무거움

초기에는 회사에서 제공하는 GitHub Copilot을 사용했습니다.
VS Code와의 통합성이 뛰어나고 익숙했지만, 설계 방향이 어긋나면서 에이전트를 재생성하거나 맥락을 다시 주입하는 과정에서 토큰 낭비와 흐름 단절이 발생했습니다.
또한 동일한 GPT-5 모델을 사용하고 있음에도 Copilot과 ChatGPT 간의 결과 품질 차이가 관찰되었습니다.

이후 Gemini CLI로 에이전트를 다시 설계했습니다.
무료 요청 한도가 넉넉해 초기 설계는 무리 없이 진행할 수 있었지만, 에이전트를 실제로 구동하는 과정에서는 리밋이 자주 걸렸고, 긴 컨텍스트를 안정적으로 이어가기 어려웠습니다.

결국 긴 컨텍스트를 안정적으로 유지하고 멀티스텝 TDD 워크플로우를 수행할 수 있는 환경이 필요했고, 이에 Cursor + Sonnet 4.5 조합을 유료 결제 후 본격적으로 사용했습니다.

Claude(Anthropic) 단독 사용도 고려했지만, Cursor 내에서 Sonnet 모델을 직접 사용할 수 있었고, 에디터 통합 + 테스트 자동화까지 지원하는 Cursor가 비용 대비 효율이 높다고 판단했습니다.

실제로 사용해보니 앞선 도구들과 다르게 프로젝트 전체를 인식하며 맥락을 일관되게 유지하는 능력이 탁월했습니다.
파일 단위의 컨텍스트 관리가 뛰어나고, 자동 리팩터링이나 테스트 주도 개발 흐름도 자연스럽게 연결되었습니다.
특히 여러 에이전트가 상태를 공유하며 점진적으로 발전하는 구조에서는 Cursor가 가장 안정적이었습니다.

이번 경험을 통해 AI의 사용 목적과 활용 범위에 따라 적합한 도구를 선택하는 것이 결과 품질에 직접적인 영향을 준다는 점을 확실히 느꼈습니다.

## 테스트를 기반으로 하는 AI를 통한 기능 개발과 없을 때의 기능개발은 차이가 있었나요?

처음에는 에이전트를 설계하고 제작하는 데 꽤 많은 시간이 들어서 “차라리 내가 직접 개발하는 게 빠르겠다”는 생각이 들었습니다.
하지만 일단 에이전트를 구동하기 시작하니 반복적이고 기계적인 작업을 빠르게 소화하면서, 개발 속도가 눈에 띄게 가속되었습니다.
테스트를 기반으로 안정적으로 개발이 진행된다는 점도 확실히 느꼈습니다.

- **AI vs 사람 단독 개발**
  - **AI**:
    요구사항 → 테스트 → 실패(RED) → 구현(GREEN) → 리팩토링(REFACTOR) 사이클이 자동화되어 회귀를 즉시 잡고, 변경 내역과 근거(테스트/커밋)가 명확히 추적되었습니다.
    환경 의존 이슈도 테스트를 통해 재현·격리되어 문제를 국소화하기 쉬웠습니다.
    이런 구조 덕분에, 추후 리팩토링 시에도 안전하게 코드를 수정할 수 있었고, AI가 무분별하게 코드를 바꾸는 것을 방지하는 일종의 가드레일로도 작동했습니다.
  - **사람 단독**:
    초기 구현 속도는 빠를 수 있지만, 엣지 케이스가 뒤늦게 드러나면 디버깅·수정 비용이 기하급수적으로 증가할 수 있습니다.
    또한 지식이 개인의 머릿속에 머물러 팀 차원의 재현성과 공유성이 떨어지는 문제도 있습니다.

- **역할 분담(무엇을 AI에게, 무엇을 사람이)**
  - **AI가 강한 일**
    - 보일러플레이트 / 반복 작업
    - 테스트 코드 초안
    - MSW 핸들러 및 목 데이터 생성
    - 대안 제시와 리팩토링 보조
    - 대규모 코드베이스 탐색·변경
  - **사람이 강한 일**
    - 문제 정의·요구사항 해석
    - 엣지 케이스 설계
    - 품질·보안·성능 판단
    - 제품 방향 및 UX 결정
    - 범위 관리와 우선순위 조정
    - 최종 검증과 승인

- **한계와 리스크**:
  에이전트 설계, 프롬프트 및 컨텍스트 준비에는 **초기 오버헤드**가 있습니다.
  모델의 환각이나 과잉 일반화 문제로 인해 테스트 설계가 곧 시스템 신뢰도의 핵심이 됩니다. 틀린 테스트는 잘못된 자동화를 낳기 때문입니다.
  긴 컨텍스트 처리의 비용, 외부 도구 신뢰성, 환경 차이 문제 등은 여전히 사람의 감시와 판단이 필요합니다.

단기나 소규모 업무처럼 명확한 해법이 이미 머릿속에 있는 경우엔 사람이 직접 개발하는 편이 더 빠를 수도 있습니다.
그러나 요구사항이 변동되거나 협업과 장기 유지보수가 전제된 상황이라면, AI+TDD가 재현성·속도·품질·추적성 측면에서 훨씬 유리했습니다.

하지만 무엇보다 인간의 개입은 필수입니다.
최종 검증자이자 테스트 설계자, 가드레일 제공자는 사람이고, AI는 그 궤도 안에서 속도를 최대화하는 엔진이라는 결론을 얻었습니다.

## AI의 응답을 개선하기 위해 추가했던 여러 정보(context)는 무엇인가요?

AI가 기능을 정확하게 구현하기 위해서는 컨텍스트를 풍부하게 제공해야 했습니다.
저는 다음과 같은 정보를 세밀하게 추가했습니다.

- 시스템 및 제약 조건: server.js 수정 금지, date-fns 라이브러리 금지, 기존 UI 재사용, 한국어 커밋 규칙, 멀티 에이전트 TDD 단계
- 코드 구조 정보: 주요 파일 경로(`src/types.ts`, `src/utils/recurringEvents.ts`, `useEventOperations.ts`)와 테스트/핸들러 구조
- 재현 정보: 콘솔 로그(날짜 계산, 업데이트/삭제 건수), 브라우저에서 재현한 절차, 실패한 테스트 로그
- API 계약: `/api/events-list`, `/api/events/:id`, `/api/recurring-events/:repeatId`의 호출 목적과 사용 규칙
- 타임존 처리 원칙: `toISOString` 사용 금지, 로컬 시간 메서드 사용
- 인간 검증 루프: AI가 생성한 코드를 직접 실행해보고, 콘솔·네트워크 탭 로그·테스트 결과를 정리해서 다시 피드백
- 에이전트 아키텍처 문서: 각 에이전트(zeus, athena, artemis, poseidon, hermes, apollo)의 역할, 입출력, 전환 조건을 명시해 일관된 협업 유지
- 운영 가이드 및 체크리스트: 커밋 메시지 규칙과 제약 준수를 AI가 어기지 않도록 `docs/guides/*`, `docs/checklists/*`를 함께 전달
- 세션 문서 묶음: `docs/sessions/<session>` 내에 `context.md`, `feature_spec.md`, `test_spec.md`, `impl_code.md` 등 워크플로우 진행 현황 및 산출물을 지속적으로 갱신하여, 현재 단계의 정확한 상태를 유지

## 이 context를 잘 활용하게 하기 위해 했던 노력이 있나요?

AI가 잘못된 방향으로 추론하지 않게 하기 위해, 명확한 범위와 기대 결과를 고정시키는 데 집중했습니다.
특히 하지 말아야 할 작업은 굵게 강조해 범위를 벗어나지 않도록 했습니다.
또한 실패 로그와 기대 결과를 함께 제시해 AI가 “무엇이 잘못됐고, 무엇이 되어야 하는가”를 명확히 인식하게 했습니다.

파일 단위로 정확한 라인을 지정해주고, API 호출 방식은 “동시 PUT 5회 X → 일괄 PUT 1회”처럼 수치로 표현해 안정성을 높였습니다.
긴 맥락은 섹션화하여 “요구사항 / 파일 / 오류 / 수정 / 테스트” 순으로 나누어 AI가 참조 포인트를 잃지 않게 했습니다.

에이전트 문서의 경우 각 단계 시작 전에 해당 가이드와 체크리스트를 반드시 첨부하고, 작업이 끝나면 산출물(명세, 테스트, 구현, 리팩토링, 컨텍스트)을 규격대로 남겨 다음 단계 입력 품질을 유지했습니다.
실제 코드가 변경된 직후에는 세션 문서를 바로 갱신해, AI가 오래된 정보를 기준으로 판단하지 않도록 관리했습니다.
마지막으로 `docs/system/agents_spec.md`(에이전트 스펙)를 컨텍스트로 함께 제공해, 각 에이전트가 워크플로우 내에서 맡는 역할/입력/출력과 Zeus 전환 조건을 명확히 하여 협업 충돌을 줄였습니다.

## 생성된 여러 결과는 만족스러웠나요? AI의 응답을 어떤 기준을 갖고 '평가(evaluation)'했나요?

전체적으로는 만족스러웠습니다.
특히 반복 일정 기능이 캘린더, 리스트, 폼에서 모두 정확히 동작했을 때 큰 성취감을 느꼈습니다.
AI의 응답은 아래 기준으로 평가했습니다.

다만 단순히 “잘 된다” 수준에서 멈추지 않고, 각 에이전트 단계가 의도한 역할을 제대로 수행했는가를 중심으로 평가했습니다.
AI의 응답은 다음 기준에 따라 검증했습니다.

- 기능 일치: 반복 아이콘, 종료일 제한, 수정·삭제 결과가 UI 전반에서 일관되게 표시되는지
- 테스트 통과: Vitest + RTL 전체 구간 통과, MSW 계약 일치 여부
- 회귀 방지: 윤년·31일·종료일 초과 등 재현 테스트 수행
- 변경 최소화: 기존 로직 재사용, 커밋 단위 준수
- 체크리스트 검증: 각 에이전트별로 체크리스트 문서를 만들어두고, 작업 완료 전 셀프체크를 수행
- 문서 일관성: 에이전트 및 세션 문서의 규격(역할·전환 조건·커밋 규칙 등)을 충족했는지

결국 이번 과정에서 얻은 가장 큰 교훈은,
AI가 아무리 정교해져도 인간의 검증이 필수적이라는 점입니다.
명확한 인풋과 평가 기준을 사람이 설계해야만, AI가 그 안에서 안정적으로 동작하고 신뢰할 수 있는 결과를 만들어낼 수 있음을 확인했습니다.

## AI에게 어떻게 질문하는것이 더 나은 결과를 얻을 수 있었나요? 시도했던 여러 경험을 알려주세요.

AI에게 질문할 때는 구체적인 예시와 로그를 함께 주는 것이 가장 효과적이었습니다.
단순히 “이 기능을 수정해줘”보다는, 입력과 기대 결과를 명확히 정의해두면 AI가 문제의 의도를 정확히 파악했습니다.

예를 들어 다음과 같이 테스트 시나리오와 기대 결과를 구체적으로 전달했습니다.

```
테스트 방식
1. 2025-11-01 ~ 2025-11-29의 1주 반복 일정 추가
2. 11/1, 11/8, 11/15, 11/22, 11/29의 일정이 표시됨
2. 11/1의 일정을 11/5일로 변경

기대 결과값
11/5, 11/12, 11/19, 11/26
(기존 11/29일은 12/3로 변경되면서 반복 완료일을 초과하기 때문에 삭제)

실제 결과값
11/5, 11/12, 11/19, 11/26, 12/3

반복 일정을 일괄적으로 변경 했을 때 반복 완료일을 초과하는 일정은 삭제처리 해줄래?
```

이처럼 기대 결과를 정확한 값으로 못 박고, 콘솔 로그와 테스트 로그 원문을 함께 제시해 탐색 범위를 좁히는 방식이 가장 효과적이었습니다.

또한 “서버 수정 불가, 프론트만 수정 가능”처럼 작업 제약 조건을 명시적으로 선언하고,
“개별 PUT vs 일괄 PUT/DELETE 시 어떤 문제가 생기는가”처럼 비교형 질문으로 AI의 추론 방향을 유도했습니다.
작업 과정은 기능 명세 → 테스트 명세 → 테스트 구현(RED) → 기능 구현(GREEN) → 리팩토링(REFACTOR) 순으로 구조화하여 불필요한 수정이나 충돌을 줄일 수 있었습니다.

또한 md 문서로 작업 지시를 제공하는 것보다, 실제 프롬프트로 요청과 지시사항을 함께 전달하는 방식이 훨씬 효과적이었습니다.
AI가 문서의 형식보다 “대화 맥락”을 더 잘 이해하기 때문에, 필요한 정보를 바로 반영하고 정확하게 응답하는 경우가 많았습니다.
그래서 중요한 사항이나 문서에 적혀 있음에도 잘 지켜지지 않았던 내용들은 프롬프트에 함께 명시하여 AI가 놓치지 않고 반영하도록 했습니다.

## AI에게 지시하는 작업의 범위를 어떻게 잡았나요? 범위를 좁게, 넓게 해보고 결과를 적어주세요. 그리고 내가 생각하는 적절한 단위를 말해보세요.

처음에는 범위를 넓게 잡아 “반복 유형 선택 → 반복 일정 표시 → 반복 종료 → 반복 일정 수정 → 반복 일정 삭제”까지 한 사이클을 통째로 시도했는데,
이 방식은 빠르게 큰 그림을 만들어낼 수 있었지만, 제약 위반(예: server.js 수정, 기존 UI 미사용)이나 예외 처리 누락(31일·윤년 등)이 자주 발생했습니다.
AI가 한 번에 너무 많은 규칙을 해석하려다 보니, 일부 세부 조건이 생략되거나 의도와 다르게 일반화되는 경우가 많았습니다.

반대로 너무 좁게 잡았을 때는 세부 동작은 정확하고 일관성이 높았지만, 전체 UX 흐름이 단절되어 캘린더나 리스트 화면까지 연결되는 시나리오를 놓치곤 했습니다.

결국 가장 적절한 단위는 “사용자 스토리 단위”로 잡는 것이었습니다.
이 정도 범위에서는 AI가 제약과 예외를 모두 인식하면서도,전체 맥락을 유지한 채 기능을 완성할 수 있었습니다.

## 동기들에게 공유하고 싶은 좋은 참고자료나 문구가 있었나요? 마음껏 자랑해주세요.

- 문구
  - “The more your tests resemble the way your software is used, the more confidence they can give you.” - Kent C. Dodds
    > “테스트가 실제 소프트웨어 사용 방식과 닮을수록, 그 테스트는 더 큰 신뢰를 줄 수 있다.” - Kent C. Dodds
  - “Write tests. Not too many. Mostly integration.” - Kent C. Dodds
    > “테스트를 작성하라. 너무 많이는 말고, 대부분은 통합 테스트로.” - Kent C. Dodds

- 자료
  - [Martin Fowler, “Mocks Aren’t Stubs” — 테스트 더블의 올바른 사용 구분](https://martinfowler.com/articles/mocksArentStubs.html):
    목(Mock)과 스텁(Stub)의 차이를 이해해, MSW 핸들러를 더 올바르게 설계할 수 있었습니다.
  - [Robert C. Martin, “The Three Laws of TDD” — TDD 3법칙](https://blog.cleancoder.com/uncle-bob/2014/12/17/TheThreeRulesOfTdd.html):
    Red → Green → Refactor 주기를 작게, 자주 반복하는 근거로 삼았습니다.
  - [Kent C. Dodds, “Guiding Principles” — Testing Library 철학](https://testing-library.com/docs/guiding-principles):
    사용자 관점에서 시나리오를 검증하는 테스트 스타일을 확립했습니다.
  - [Google Testing Blog — 대규모 테스트 실전 사례 모음](https://testing.googleblog.com/):
    flaky test 대응과 테스트 피라미드 설계에 참고했습니다.
  - [Jez Humble, “Continuous Delivery” — 배포 파이프라인과 자동화 테스트](https://continuousdelivery.com/):
    단계별 커밋과 자동화 테스트 운영 철학을 이번 과제의 기반으로 삼았습니다.

## AI가 잘하는 것과 못하는 것에 대해 고민한 적이 있나요? 내가 생각하는 지점에 대해 작성해주세요.

AI는 반복적이고 패턴화된 작업이나 대규모 코드 리팩토링에 매우 강했습니다.
특히 동일한 규칙을 여러 파일에 적용하거나 테스트 코드 초안을 빠르게 작성할 때, 사람보다 훨씬 빠르고 일관되게 처리할 수 있었습니다.
또한 테스트 실패 로그를 기반으로 회귀 원인을 추적하는 데도 유용했고, 단순한 로직 누락이나 날짜 계산 실수 같은 문제는 거의 즉시 찾아냈습니다.

반대로, 요구사항의 의도를 해석하거나 복잡한 데이터 정합성을 판단해야 하는 부분에서는 한계를 느꼈습니다.
예를 들어, 성능과 안정성, 보안과 편의성 간의 트레이드오프가 필요한 결정에서는 AI가 스스로 최적 방향을 판단하지 못했고, 사람이 직접 기준을 잡아야 했습니다.
특히 브라우저 환경에서 발생하는 비동기 타이밍 문제나 실제 운영 환경 특화 버그는, AI가 로그를 확인하더라도 “왜 발생했는지”까지 이해하지 못했습니다.

정리하면, AI와 사람은 서로 강점이 다릅니다.

- AI: 속도, 반복 작업, 대규모 코드 처리, 테스트 기반 회귀 추적
- 사람: 방향 설정, 의도 해석, 트레이드오프 판단, 최종 검증

AI와 사람의 역할을 명확히 구분하고 서로 보완할 때, 개발의 효율과 안정성이 모두 향상됩니다.
AI는 반복과 속도를 담당하고, 사람은 방향과 판단을 맡아 협업할 때 최적의 결과를 낼 수 있다고 느꼈습니다.

## 마지막으로 느낀점에 대해 적어주세요!

처음에는 멀티에이전트 구조를 설계하고 컨텍스트를 정리하는 과정이 너무 복잡하고 번거롭게 느껴졌습니다.
하지만 일단 구조가 잡히고 나서는 개발 속도와 품질, 그리고 추적성이 눈에 띄게 향상되어 진가를 체험했습니다.

특히 TDD 기반의 단계적 커밋과 AI 오케스트레이션을 병행했을 때, 기능 추가 속도는 빨라지면서도 품질이 안정적으로 유지됐습니다.
테스트 자동화 덕분에 “이전 기능이 망가졌을까?”라는 불안을 거의 느끼지 않았고, 일괄 API 전략을 적용하면서 안정성을 높일 수 있었습니다.

무엇보다도 가장 크게 배운 점은 AI를 단순한 ‘자동화 도구’로 두지 않고, 명확한 규칙과 테스트로 감싸주는 ‘협업 파트너’로 다루는 게 핵심이라는 점이었습니다.

이번 과제를 통해 깨달은 결론은 명확합니다.

“사람이 가드레일을 깔고, AI가 가속한다.”
이 구조야말로 앞으로의 안정적인 AI 기반 개발에 가장 잘 맞는 형태라고 생각합니다.
