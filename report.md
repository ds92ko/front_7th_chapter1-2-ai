# AI와 테스트를 활용한 안정적인 기능 개발 리포트

## 사용하는 도구를 선택한 이유가 있을까요? 각 도구의 특징에 대해 조사해본적이 있나요?

이번 과제는 비용을 최대한 절약하는 방향으로 시작했습니다.
먼저 회사에서 제공하는 GitHub Copilot로 진행했는데, 인라인 코드 보조에는 강점이 있지만 복수 단계의 설계·테스트·구현을 일관되게 끌고 가는 에이전트 워크플로우엔 한계가 있어, 제가 초기에 잡은 설계가 빗나가자 몇 번 만든 에이전트를 삭제/재생성하는 과정에서 토큰을 소모만 하고 성과가 떨어졌습니다.

사전 조사와 실제 사용을 병행하며 결과 품질(정답성/일관성), 실행 가능한 코드 산출 능력, 길고 복잡한 컨텍스트 유지력, 리밋/안정성, 비용, 에디터 통합성 등을 기준으로 도구를 비교했습니다

- GitHub Copilot
  - IDE 통합과 짧은 코드 생성은 매우 빠른 편
  - 장문의 명세 기반 TDD 파이프라인(테스트 → 구현 → 리팩토링)의 단계적 진행과 의도 보존은 약함
- Google Gemini(Pro)
  - 지식 커버리지와 비용 대비 속도는 좋았고 문서화/요약에 강점
  - 다만 동일 프롬프트를 여러 번 돌려도 원하는 아키텍처 수준의 산출이 일관되게 나오지 않았고, 에이전트 구동을 반복하다 리밋(쿼터)에 걸리는 빈도가 높아 실전 개발 흐름이 끊김
- Cursor + Sonnet(4.5)
  - 에디터 내 컨텍스트 공유, 긴 코드베이스 다루기, 테스트 주도 워크플로우(파일 수정 → 테스트 → 커밋) 자동화를 매끄럽게 지원
  - 긴 컨텍스트와 도구 호출에 안정적이고, 멀티스텝 지시를 그대로 수행해 바로 제가 의도한 결과를 내는 비율이 높았음

현실적인 제약도 선택에 큰 영향을 줬습니다.
최대한 무료로 도전하려고 Copilot과 Gemini를 우선 활용했지만, 설계 보정과 재시도가 잦아 토큰/리밋 이슈를 반복적으로 만났고, 결국 안정적인 실행을 위해 Cursor를 유료 결제하여 에이전트를 구동했습니다.
Claude는 품질은 높지만 Cursor에서 Sonnet 4.5 모델 사용이 가능했고, 예산 기준에서 비용 효율이 낮다고 판단해 제외했습니다.

요약하면, 간단한 보일러플레이트 생성·수정엔 Copilot이, 문서화·요약엔 Gemini가 도움이 됐고, 최종적으로는 긴 맥락을 보존한 채 TDD 파이프라인을 일관되게 실행해 주는 Cursor(Sonnet)가 가장 생산적이었습니다.
특히 Cursor는 같은 명세로 여러 번 돌려도 결과 편차가 적고, 테스트와 코드 편집/커밋을 한 흐름으로 이어주는 점이 이번 과제에 결정적이었습니다.

## 테스트를 기반으로 하는 AI를 통한 기능 개발과 없을 때의 기능개발은 차이가 있었나요?

처음에는 에이전트를 설계하고 제작하는 데 꽤 많은 시간이 들어서 “차라리 내가 직접 코딩하는 게 빠르겠다”는 생각이 들었습니다.
하지만 일단 에이전트를 구동하기 시작하니 반복·기계적 작업을 빠르게 소화하면서 개발이 눈에 띄게 가속됐습니다.

- **AI vs 사람 단독 개발**
  - **AI**
    - 요구사항 → 테스트 → 실패(Red) → 구현(Green) → 리팩토링(Refactor) 사이클이 자동화되어 회귀를 즉시 잡고, 변경 내역과 근거(테스트/커밋)가 추적 가능했습니다
    - 타임존 / 레이스컨디션 같은 환경 의존 이슈도 테스트로 재현·격리되어 문제를 국소화하기 쉬웠습니다.
  - **사람 단독**
    - 초기 구현 속도는 빠를 수 있지만, 모서리 케이스가 뒤늦게 드러나면 디버깅·수정 비용이 기하급수적으로 증가할 것으로 예상됩니다.
    - 지식이 개인 머릿속에 머물러 팀 공유성과 재현성이 낮아지는 문제도 있습니다.

- **역할 분담(무엇을 AI에게, 무엇을 사람이)**
  - **AI가 강한 일**
    - 보일러플레이트/반복 작업
    - 테스트 코드 초안
    - MSW 핸들러/목 생성
    - 대안 제시와 리팩토링 보조
    - 대규모 코드베이스 탐색·변경
  - **사람이 강한 일**
    - 문제 정의·요구사항 해석
    - 엣지 케이스 설계
    - 품질/보안/성능 판단
    - 제품·UX 의사결정
    - 범위 관리와 우선순위 조정
    - 최종 검증과 승인

- **한계와 리스크**
  - 에이전트 설계/프롬프트·컨텍스트 준비의 **초기 오버헤드**는 현실적입니다.
  - 모델의 환각/과잉 일반화 리스크가 있어, 테스트 설계가 중요하고, 틀린 테스트는 **틀린 자동화**가 되어버립니다.
  - 긴 컨텍스트 처리의 **리밋/비용**, 외부 도구 신뢰성, 상태 동기화/환경 편차(예: 타임존) 이슈는 여전히 사람의 감시가 필요합니다.

- **이번 과제에서의 구체적 차이**
  - 반복 일정 수정·삭제
    - 프론트 단에서 날짜 동기화/종료일 준수/레이스컨디션을 해결
    - 테스트가 PUT/DELETE 호출 순서·개수
    - 일괄 API 사용 여부를 검증해 회귀 차단
  - 타임존 버그
    - `toISOString`으로 인한 날짜 시프트를 로컬 메서드로 교체
    - 유닛/통합 테스트가 재발 방지 장치가 됨
  - 간단한 UI
    - AI가 즉시 구현했고, 통합 테스트로 안전하게 확인

단기·소규모 업무나 이미 머릿속에 해법이 명확할 때는 사람 단독 개발이 더 빠를 수 있습니다.
그러나 **요구사항 변동/협업/장기 유지보수**가 전제되면, **AI+TDD가 재현성·속도·품질·추적성**에서 확실히 유리하다고 판단했습니다.
하지만 무엇보다 **인간의 개입은 필수**입니다.
최종 검증자이자 테스트 설계자, 가드레일 제공자는 사람이고, AI는 그 궤도 안에서 속도를 최대화하는 엔진이라는 결론을 얻었습니다.

## AI의 응답을 개선하기 위해 추가했던 여러 정보(context)는 무엇인가요?

AI가 기능을 정확하게 구현하기 위해서는 컨텍스트를 풍부하게 제공해야 했습니다.
저는 다음과 같은 정보를 세밀하게 추가했습니다.

- 시스템 및 제약 조건: server.js 수정 금지, date-fns 라이브러리 금지, 기존 UI 재사용, 한국어 커밋 규칙, 멀티 에이전트 TDD 단계
- 코드 구조 정보: 주요 파일 경로(`src/types.ts`, `src/utils/recurringEvents.ts`, `useEventOperations.ts`)와 테스트/핸들러 구조
- 재현 정보: 콘솔 로그(날짜 계산, 업데이트/삭제 건수), 브라우저에서 재현한 절차, 실패한 테스트 로그
- API 계약: `/api/events-list`, `/api/events/:id`, `/api/recurring-events/:repeatId`의 호출 목적과 사용 규칙
- 타임존 처리 원칙: `toISOString` 사용 금지, 로컬 시간 메서드 사용
- 인간 검증 루프: AI가 생성한 코드를 직접 실행해보고, 콘솔·네트워크 탭 로그·테스트 결과를 정리해서 다시 피드백
- 에이전트 아키텍처 문서: 각 에이전트(zeus, athena, artemis, poseidon, hermes, apollo)의 역할, 입출력, 전환 조건을 명시해 일관된 협업 유지
- 운영 가이드 및 체크리스트: 커밋 메시지 규칙과 제약 준수를 AI가 어기지 않도록 `docs/guides/*`, `docs/checklists/*`를 함께 전달
- 세션 문서 묶음: `docs/sessions/<session>` 내에 `context.md`, `feature_spec.md`, `test_spec.md`, `impl_code.md` 등 워크플로우 진행 현황 및 산출물을 지속적으로 갱신하여, 현재 단계의 정확한 상태를 유지

## 이 context를 잘 활용하게 하기 위해 했던 노력이 있나요?

AI가 잘못된 방향으로 추론하지 않게 하기 위해, 명확한 범위와 기대 결과를 고정시키는 데 집중했습니다.
특히 하지 말아야 할 작업은 굵게 강조해 범위를 벗어나지 않도록 했습니다.
또한 실패 로그와 기대 결과를 함께 제시해 AI가 “무엇이 잘못됐고, 무엇이 되어야 하는가”를 명확히 인식하게 했습니다.

파일 단위로 정확한 라인을 지정해주고, API 호출 방식은 “동시 PUT 5회 X → 일괄 PUT 1회”처럼 수치로 표현해 레이스컨디션을 제거했습니다.
긴 맥락은 섹션화하여 “요구사항 / 파일 / 오류 / 수정 / 테스트” 순으로 나누어 AI가 참조 포인트를 잃지 않게 했습니다.

에이전트 문서의 경우 각 단계 시작 전에 해당 가이드와 체크리스트를 반드시 첨부하고, 작업이 끝나면 산출물(명세, 테스트, 구현, 리팩토링, 컨텍스트)을 규격대로 남겨 다음 단계 입력 품질을 유지했습니다.
실제 코드가 변경된 직후에는 세션 문서를 바로 갱신해, AI가 오래된 정보를 기준으로 판단하지 않도록 관리했습니다.
마지막으로 `docs/system/agents_spec.md`(에이전트 스펙)를 컨텍스트로 함께 제공해, 각 에이전트가 워크플로우 내에서 맡는 역할/입력/출력과 Zeus 전환 조건을 명확히 하여 협업 충돌을 줄였습니다.

## 생성된 여러 결과는 만족스러웠나요? AI의 응답을 어떤 기준을 갖고 '평가(evaluation)'했나요?

전체적으로는 만족스러웠습니다.
특히 반복 일정 기능이 캘린더, 리스트, 폼에서 모두 정확히 동작했을 때 큰 성취감을 느꼈습니다.
AI의 응답은 아래 기준으로 평가했습니다.

- 기능 일치: 반복 아이콘, 종료일 제한, 수정·삭제 결과가 UI 전반에서 일관되게 표시되는지
- 테스트 통과: Vitest + RTL 전체 구간 통과, MSW 계약 일치 여부
- 회귀 방지: 타임존·윤년·31일·종료일 초과·레이스컨디션 등 재현 테스트 수행
- 변경 최소화: 기존 로직 재사용, 커밋 단위 준수
- 효율: 개별 호출 대신 일괄 처리로 안정성 확보
- 문서 일관성: 에이전트 및 세션 문서의 규격(역할·전환 조건·커밋 규칙)을 충족했는지

AI 모델은 종종 환각이나 과잉 일반화를 일으킬 수 있습니다.
그래서 테스트가 곧 안전망이었습니다.
테스트가 틀리면 잘못된 코드가 그대로 퍼지는 **‘틀린 자동화’**가 되기 때문에,
테스트 자체를 먼저 신뢰성 있게 만드는 것이 핵심이었습니다.

## AI에게 어떻게 질문하는것이 더 나은 결과를 얻을 수 있었나요? 시도했던 여러 경험을 알려주세요.

AI에게 질문할 때는 구체적인 예시와 로그를 함께 주는 것이 가장 효과적이었습니다.
예를 들어 “11월 1일부터 5일까지 반복 일정을 이동했을 때 캘린더에 어떻게 표시되어야 하는가”처럼
기대 결과를 수치로 못 박았습니다.
또한 콘솔 로그와 테스트 로그 원문을 함께 제시해 탐색 범위를 좁혔습니다.

“서버 수정 불가”, “프론트만 수정 가능” 같은 제약을 명시적으로 선언했고,
“개별 PUT vs 일괄 PUT/DELETE 시 어떤 문제가 생기는가”처럼 비교 질문을 통해 AI의 추론력을 검증했습니다.
작업은 기능 명세 → 테스트 명세 → 테스트 구현(RED) → 기능 구현(GREEN) → 리팩토링(REFACTOR) 순으로 분리해, 충돌이나 불필요한 수정이 줄었습니다.

## AI에게 지시하는 작업의 범위를 어떻게 잡았나요? 범위를 좁게, 넓게 해보고 결과를 적어주세요. 그리고 내가 생각하는 적절한 단위를 말해보세요.

처음에는 범위를 넓게 잡아 “반복 유형 선택 → 반복 일정 표시 → 반복 종료 → 반복 일정 수정 → 반복 일정 삭제”까지 한 사이클을 통째로 시도했는데,
이 방식은 빠르게 큰 그림을 만들어낼 수 있었지만, 제약 위반(예: server.js 수정, 기존 UI 미사용)이나 예외 처리 누락(31일·윤년 등)이 자주 발생했습니다.
AI가 한 번에 너무 많은 규칙을 해석하려다 보니, 일부 세부 조건이 생략되거나 의도와 다르게 일반화되는 경우가 많았습니다.

반대로 너무 좁게 잡았을 때는 세부 동작은 정확하고 일관성이 높았지만, 전체 UX 흐름이 단절되어 캘린더나 리스트 화면까지 연결되는 시나리오를 놓치곤 했습니다.

결국 가장 적절한 단위는 “사용자 스토리 단위”로 잡는 것이었습니다.
이 정도 범위에서는 AI가 제약과 예외를 모두 인식하면서도,전체 맥락을 유지한 채 기능을 완성할 수 있었습니다.

## 동기들에게 공유하고 싶은 좋은 참고자료나 문구가 있었나요? 마음껏 자랑해주세요.

- 문구
  - Kent C. Dodds: “The more your tests resemble the way your software is used, the more confidence they can give you.”
  - Kent C. Dodds: “Write tests. Not too many. Mostly integration.”

- 자료
  - Martin Fowler, “Mocks Aren’t Stubs” — 테스트 더블의 올바른 사용 구분 [링크](https://martinfowler.com/articles/mocksArentStubs.html)
    목(Mock)과 스텁(Stub)의 차이를 이해해, MSW 핸들러를 더 올바르게 설계할 수 있었습니다.
  - Robert C. Martin, “The Three Laws of TDD” — TDD 3법칙 [링크](https://blog.cleancoder.com/uncle-bob/2014/12/17/TheThreeRulesOfTdd.html)
    Red → Green → Refactor 주기를 작게, 자주 반복하는 근거로 삼았습니다.
  - Kent C. Dodds, “Guiding Principles” — Testing Library 철학 [링크](https://testing-library.com/docs/guiding-principles)
    사용자 관점에서 시나리오를 검증하는 테스트 스타일을 확립했습니다.
  - Google Testing Blog — 대규모 테스트 실전 사례 모음 [링크](https://testing.googleblog.com/)
    flaky test 대응과 테스트 피라미드 설계에 참고했습니다.
  - Jez Humble, “Continuous Delivery” — 배포 파이프라인과 자동화 테스트 [링크](https://continuousdelivery.com/)
    단계별 커밋과 자동화 테스트 운영 철학을 이번 과제의 기반으로 삼았습니다.

## AI가 잘하는 것과 못하는 것에 대해 고민한 적이 있나요? 내가 생각하는 지점에 대해 작성해주세요.

AI는 패턴화된 반복 작업이나 대규모 코드 리팩토링에 정말 강했습니다.
특히 동일한 규칙을 여러 파일에 적용해야 할 때나 테스트 코드 초안을 빠르게 뽑을 때, 사람보다 훨씬 빠르고 일관성 있게 처리해줬습니다.
테스트 실패 로그를 기반으로 회귀 원인을 추적하는 데도 유용했습니다.
단순한 로직 누락이나 날짜 계산 실수 같은 건 AI가 거의 즉시 찾아냈습니다.

하지만 반대로, 요구사항의 ‘의도’를 해석해야 하는 부분에서는 자주 한계를 느꼈습니다.
또한 데이터 정합성이나 트레이드오프 판단(예: 성능 ↔ 안정성, 보안 ↔ 편의성)에서는 맥락을 추론하지 못해 사람이 직접 방향을 잡아줘야 했습니다.
특히 실제 브라우저 환경에서 발생하는 타임존 문제나 비동기 타이밍 이슈는 AI가 로그를 봐도 “왜” 생기는지까지는 스스로 이해하지 못했습니다.

결론적으로 저는 이렇게 정리했습니다.

AI는 속도와 확장성에 강하고, 사람은 방향·판단·검증에 강하다.
둘의 역할을 명확히 구분하고 조합할 때, 개발의 효율과 안정성이 모두 올라간다.

## 마지막으로 느낀점에 대해 적어주세요!

처음에는 처음에는 멀티에이전트 구조를 설계하고 컨텍스트를 정리하는 과정이 너무 복잡하고 번거롭게 느껴졌습니다.
하지만 일단 구조가 잡히고 나서는 개발 속도와 품질, 그리고 추적성이 눈에 띄게 향상되어 진가를 체험했습니다.

특히 TDD 기반의 단계적 커밋과 AI 오케스트레이션을 병행했을 때, 기능 추가 속도는 빨라지면서도 품질이 안정적으로 유지됐습니다.
테스트 자동화 덕분에 “이전 기능이 망가졌을까?”라는 불안을 거의 느끼지 않았고,
일괄 API 전략을 적용하면서 레이스컨디션이나 타임존 오류 같은 현실적인 문제들도 제어 가능한 수준으로 정리할 수 있었습니다.

무엇보다도 가장 크게 배운 점은 AI를 단순한 ‘자동화 도구’로 두지 않고, 명확한 규칙과 테스트로 감싸주는 ‘협업 파트너’로 다루는 게 핵심이라는 점이었습니다.

이번 과제를 통해 깨달은 결론은 명확합니다.

“사람이 가드레일을 깔고, AI가 가속한다.”
이 구조야말로 앞으로의 안정적인 AI 기반 개발에 가장 잘 맞는 형태라고 생각합니다.
